{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##RAG Privacy Policy Simplifier\n",
        "\n",
        "\n",
        "**Project type:** Proof-of-Concept (PoC)  \n",
        "**Input:** fittiching Privacy policy URL online  \n",
        "**Output:** Simplified rewrite in clear English  \n",
        "**Main idea:** Use **RAG** to reduce hallucination and summarization and highlighting\n"
      ],
      "metadata": {
        "id": "1SVI-N249ZAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Problem Statement\n",
        "\n",
        "Privacy policies are tooo long and written in legal language. Most users dont read them fully or they misunderstand what data is collected, how it is used, and who it is shared with. Normal summarizers can also **hallucinate** or **drop important details**"
      ],
      "metadata": {
        "id": "4L_AZiCb-_Tl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Goal\n",
        "\n",
        "Build a lightweight end-to-end **RAG pipeline** that rewrites (not invents) or omit privacy policy text into **clear, simple English** while preserving meaning and reducing hallucination risk. This is mainly to **prove the idea works** before heavy optimization. With focus **on only TikTok** as a PoC use case for simplifying.\n"
      ],
      "metadata": {
        "id": "TjSbJ8Aa_Kim"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Scope\n",
        "\n",
        "**Included:**\n",
        "- Fetch policy text from official URL\n",
        "- Clean text (fix weird encoding chars)\n",
        "- Chunking + embeddings + retrieval (Top-K)\n",
        "- Rewrite with strict rules (no guessing)\n",
        "- evalution\n",
        "\n",
        "**Not included (for now):**\n",
        "- Legal verification / lawyer review\n",
        "- Perfect section-by-section formatting\n",
        "- Multi-doc evaluation system (advanced scoring)\n",
        "- Guarantee of 100% coverage\n"
      ],
      "metadata": {
        "id": "Sav7Sr7o_fGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Development and Coding**"
      ],
      "metadata": {
        "id": "NtSoVyxHBofw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries\n",
        "\n"
      ],
      "metadata": {
        "id": "iKDRHmxRRSTZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7UtPdT-IgWl"
      },
      "outputs": [],
      "source": [
        "!pip -q install -U transformers accelerate bitsandbytes sentence-transformers lxml textstat matplotlib\n",
        "!pip -q install pandas==2.2.2 requests==2.32.4\n",
        "\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import textstat\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "if device == \"cuda\":\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Secure HF Token + Project Configration"
      ],
      "metadata": {
        "id": "loxAUcNARr4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "if not os.getenv(\"HF_TOKEN\"):\n",
        "    print(\"\\nHF_TOKEN not found in environment.\")\n",
        "    print(\"Paste your HuggingFace token:\")\n",
        "    os.environ[\"HF_TOKEN\"] = getpass(\"HF_TOKEN: \")\n",
        "\n",
        "hf_token = os.getenv(\"HF_TOKEN\", \"\")\n",
        "\n",
        "print(\"HF_TOKEN check passed.\")\n",
        "\n",
        "POLICY_URLS = [\n",
        "    \"https://www.tiktok.com/legal/privacy-policy?lang=en\"\n",
        "]\n",
        "\n",
        "RETRIEVAL_QUERY = \"data collection, sharing, retention, rights, security, advertising, tracking\"\n",
        "\n",
        "# Chunking knobs\n",
        "CHUNK_WORDS = 500  # after lots of trials this was the best threshould for the use case\n",
        "CHUNK_OVERLAP = 80\n",
        "MIN_CHUNK_WORDS = 120\n",
        "\n",
        "# Retrieval knobs\n",
        "TOP_K = 12  # this is after alot of trials i started with 6 but if you prefer more coverage, ok\n",
        "\n",
        "# Models\n",
        "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "GEN_MODEL_NAME   = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "# Generator knobs\n",
        "GEN_MAX_NEW_TOKENS = 1400 # to make it stable\n"
      ],
      "metadata": {
        "id": "Xag8P20gLZy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fetching Polices + Generic cleaning"
      ],
      "metadata": {
        "id": "GqK8BgcbTFuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _normalize_whitespace(s: str) -> str:\n",
        "    s = s.replace(\"\\u00a0\", \" \")\n",
        "    s = re.sub(r\"[ \\t]+\", \" \", s)\n",
        "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
        "    return s.strip()\n",
        "\n",
        "# This removes the unreadable symbols retrieved while reading the policies, which I observed after reviewing a number of them.\n",
        "def fix_mojibake(s: str) -> str:\n",
        "    return (s.replace(\"â\", \"'\")\n",
        "             .replace(\"â\", '\"')\n",
        "             .replace(\"â\", '\"')\n",
        "             .replace(\"â\", \"-\")\n",
        "             .replace(\"Â\", \" \")\n",
        "             .strip())\n",
        "\n",
        "def fetch_policy(url: str, timeout: int = 30) -> str:\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0\",\n",
        "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "    }\n",
        "    r = requests.get(url, headers=headers, timeout=timeout)\n",
        "    if r.status_code != 200:\n",
        "        preview = (r.text or \"\")[:200]\n",
        "        raise RuntimeError(f\"Fetch failed: {r.status_code}\\nPreview:\\n{preview}\")\n",
        "\n",
        "    soup = BeautifulSoup(r.text, \"lxml\")\n",
        "\n",
        "    for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
        "        tag.decompose()\n",
        "\n",
        "    main = soup.find(\"main\") or soup.find(\"article\")\n",
        "    text = main.get_text(\"\\n\") if main else soup.get_text(\"\\n\")\n",
        "\n",
        "    text = _normalize_whitespace(text)\n",
        "    text = fix_mojibake(text)\n",
        "    return text\n",
        "\n",
        "def clean_text(raw: str) -> str:\n",
        "    before = len(raw)\n",
        "    lines = [ln.strip() for ln in raw.splitlines()]\n",
        "    lines = [ln for ln in lines if len(ln) > 2]\n",
        "    cleaned = \"\\n\".join(lines)\n",
        "    cleaned = _normalize_whitespace(cleaned)\n",
        "    cleaned = fix_mojibake(cleaned)\n",
        "    after = len(cleaned)\n",
        "    print(f\"Cleaned chars: {after} (removed ~{max(0, before-after)} chars)\")\n",
        "    return cleaned\n",
        "\n",
        "# Will be used to display the result in consle for faster viewing\n",
        "def preview_head_tail(text: str, n: int = 1200) -> None:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"TEXT STATS | chars: {len(text)} | words: {len(text.split())}\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\n[HEAD PREVIEW]\\n\")\n",
        "    print(text[:n])\n",
        "    print(\"\\n\" + \"-\"*70)\n",
        "    print(\"\\n[TAIL PREVIEW]\\n\")\n",
        "    print(text[-n:])\n",
        "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "\n",
        "policies = []\n",
        "for url in POLICY_URLS:\n",
        "    print(\"\\nFetching:\", url)\n",
        "    raw = fetch_policy(url)\n",
        "    text = clean_text(raw)\n",
        "\n",
        "    if len(text) < 20000:\n",
        "        print(\"WARNING: Policy text looks too short (<20k chars). It may be incomplete\")\n",
        "\n",
        "    preview_head_tail(text, n=1200)\n",
        "    policies.append({\"url\": url, \"text\": text})\n",
        "\n",
        "# Optional manual override: because after alot of trials many of polcies are pages are protected from fitiching\n",
        "MANUAL_POLICY_TEXT = \"\"\n",
        "if MANUAL_POLICY_TEXT.strip():\n",
        "    policies = [{\"url\": \"MANUAL_INPUT\", \"text\": clean_text(MANUAL_POLICY_TEXT)}]\n",
        "    preview_head_tail(policies[0][\"text\"], n=1200)\n"
      ],
      "metadata": {
        "id": "4syXZeUjLd0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I pulled the TikTok policy text from the link and cleaned the messy lines.\n",
        "I checked the **head** and **tail** to make sure the page is not empty or cut.\n",
        "It looks full length (big **word count**), so i can move to chunking now.\n"
      ],
      "metadata": {
        "id": "gHWx8c6YL9Hp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**(RAG pipeline)**"
      ],
      "metadata": {
        "id": "C71gb_xwvmIj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Chunking\n",
        "split policy into overlapping chunks\n"
      ],
      "metadata": {
        "id": "z5rGLMoCT6pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text: str, chunk_words: int = 500, overlap_words: int = 80, min_words: int = 120) -> list[str]:\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    i = 0\n",
        "    step = max(1, chunk_words - overlap_words)\n",
        "\n",
        "    while i < len(words):\n",
        "        chunk_words_list = words[i:i + chunk_words]\n",
        "        chunk = \" \".join(chunk_words_list).strip()\n",
        "        if chunk and len(chunk_words_list) >= min_words:\n",
        "            chunks.append(chunk)\n",
        "        i += step\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def chunk_stats(chunks: list[str]) -> None:\n",
        "    sizes = [len(c.split()) for c in chunks]\n",
        "    print(f\"Chunks: {len(chunks)} | words min/avg/max: {min(sizes)}/{sum(sizes)//len(sizes)}/{max(sizes)}\")\n",
        "\n",
        "def preview_chunks(chunks: list[str], take: int = 2, chars: int = 800) -> None:\n",
        "    take = min(take, len(chunks))\n",
        "    for idx in range(take):\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(f\"CHUNK #{idx} | words: {len(chunks[idx].split())}\")\n",
        "        print(\"=\"*70)\n",
        "        print(chunks[idx][:chars])\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "for p in policies:\n",
        "    chunks = chunk_text(p[\"text\"], CHUNK_WORDS, CHUNK_OVERLAP, MIN_CHUNK_WORDS)\n",
        "\n",
        "    if len(chunks) < 6:\n",
        "        print(\"WARNING: Few chunks (<6). !!!!\")\n",
        "\n",
        "    chunk_stats(chunks)\n",
        "    preview_chunks(chunks, take= 2, chars= 800)\n",
        "    p[\"chunks\"] = chunks\n"
      ],
      "metadata": {
        "id": "VxBcpe9dL90E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "look i chunked the policy into ~500-word parts with a small **overlap** so the edges dont cut meanings\n",
        "ended up with **20 chunks**, most of them near the same size, so retrieval should be stable i think\n",
        "the small last chunk is normal,"
      ],
      "metadata": {
        "id": "QKwS7Cu4MiJ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Embeddings + Retrieval"
      ],
      "metadata": {
        "id": "dfoqMcl-TbXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedder = SentenceTransformer(EMBED_MODEL_NAME)\n",
        "\n",
        "def embed_chunks(chunks: list[str]) -> tuple[np.ndarray, list[str]]:\n",
        "    chunks_clean = [c.strip() for c in chunks if c and c.strip()]\n",
        "    emb = embedder.encode(chunks_clean, normalize_embeddings=True, show_progress_bar=False)\n",
        "    return np.array(emb), chunks_clean\n",
        "\n",
        "def retrieve_topk(chunks: list[str], chunk_emb: np.ndarray, query: str, top_k: int):\n",
        "    q_emb = embedder.encode([query], normalize_embeddings=True, show_progress_bar=False)[0]\n",
        "    scores = chunk_emb @ q_emb\n",
        "    k = min(top_k, len(scores))\n",
        "    top_idx = np.argsort(-scores)[:k].tolist()\n",
        "    return top_idx, [chunks[i] for i in top_idx], [float(scores[i]) for i in top_idx]\n",
        "\n",
        "def preview_retrieved(indices, retrieved, scores, chars: int = 800):\n",
        "    print(\"\\nRetrieved chunk indices:\", indices)\n",
        "    for j, (idx, txt, sc) in enumerate(zip(indices, retrieved, scores)):\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(f\"RETRIEVED #{j} | chunk={idx} | score={sc:.4f} | words={len(txt.split())}\")\n",
        "        print(\"=\" * 70)\n",
        "        print(txt[:chars])\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "\n",
        "for p in policies:\n",
        "    p[\"chunk_emb\"], p[\"chunks_clean\"] = embed_chunks(p[\"chunks\"])\n",
        "\n",
        "    print(f'\\n[RAG] url={p.get(\"url\",\"\")} | chunks={len(p[\"chunks_clean\"])} | TOP_K={TOP_K}')\n",
        "    idxs, retrieved, scores = retrieve_topk(p[\"chunks_clean\"], p[\"chunk_emb\"], RETRIEVAL_QUERY, TOP_K)\n",
        "\n",
        "    preview_retrieved(idxs, retrieved, scores, chars=800)\n",
        "\n",
        "    p[\"retrieved_idxs\"] = idxs\n",
        "    p[\"retrieved_texts\"] = retrieved\n",
        "    p[\"retrieved_scores\"] = scores\n"
      ],
      "metadata": {
        "id": "aV3hZThQMisU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "look i ran embeddings on the **20 chunks** and pulled **TOP_K=12**\n",
        "top results look real tbh: sharing, ads/analytics, location, security, transfers, contacts, purchases, and rights\n",
        "\n",
        "scores are mostly ~0.50 which is fine, and the few lower ones are just country add-on parts, not random junk or noise\n",
        "\n",
        "now i can feed these retrieved chunks into the generator\n"
      ],
      "metadata": {
        "id": "7Y5cjasvNTOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Generator Model (Mistral 7B Instruct, 4-bit)\n",
        "\n",
        "Load tokenizer + model\n",
        "\n",
        "4-bit quantization to fit GPU"
      ],
      "metadata": {
        "id": "qh7fIICxOHOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(GEN_MODEL_NAME, token=hf_token, use_fast=True)\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    GEN_MODEL_NAME,\n",
        "    token=hf_token,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "print(\"Loaded generator:\", GEN_MODEL_NAME)\n",
        "print(\"pad_token_id:\", tokenizer.pad_token_id)\n"
      ],
      "metadata": {
        "id": "DO7Tv5RENTyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I loaded the **generator model** (Mistral 7B Instruct) and it finished without errors\n",
        "So now the model is ready"
      ],
      "metadata": {
        "id": "01G4y6SkO8k-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Rewrite/Simplify"
      ],
      "metadata": {
        "id": "SYvItfX1U-w2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_context_from_idxs(all_chunks: list[str], idxs: list[int], max_chunks: int = 11, per_chunk_chars: int = 1200) -> str:\n",
        "    uniq = sorted(set(idxs))[:max_chunks]\n",
        "    chosen = []\n",
        "    for i in uniq:\n",
        "        c = (all_chunks[i] or \"\").strip()\n",
        "        c = fix_mojibake(c)\n",
        "        if c:\n",
        "            chosen.append(c[:per_chunk_chars])\n",
        "    return \"\\n\\n\".join(chosen)\n",
        "\n",
        "def cleanup_output(text: str) -> str:\n",
        "    t = fix_mojibake(text.replace(\"\\r\", \"\")).strip()\n",
        "    t = re.sub(r\"\\n{3,}\", \"\\n\\n\", t)\n",
        "    t = re.sub(r\"\\.{2,}\", \".\", t)      # remove \".. .. ..\"\n",
        "    t = t.replace(\"<\", \"\").replace(\">\", \"\")\n",
        "    return t.strip()\n",
        "\n",
        "def generate_rewrite(context: str, max_new_tokens: int = 1400) -> str:\n",
        "    user_prompt = (\n",
        "        \"Task: Rewrite this privacy policy in clear, simple English for an average user.\\n\\n\"\n",
        "        \"Strict rules:\\n\"\n",
        "        \"- Preserve the meaning. Do not add, guess, or invent anything.\\n\"\n",
        "        \"- Do not merge different parts in a way that changes meaning.\\n\"\n",
        "        \"- If something is unclear or not explicitly stated, omit it.\\n\"\n",
        "        \"- Use short paragraphs only. No bullet lists. No numbering.\\n\"\n",
        "        \"- Avoid unusual symbols. Use normal punctuation only.\\n\"\n",
        "        \"- Do not repeat the same idea again and again. If you start repeating, stop.\\n\\n\"\n",
        "        \"Text:\\n\"\n",
        "        f\"{context}\\n\"\n",
        "    )\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": user_prompt}]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True) if hasattr(tokenizer, \"apply_chat_template\") else user_prompt\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=5500)\n",
        "    input_len = inputs[\"input_ids\"].shape[-1]\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,\n",
        "            repetition_penalty=1.12, # After trying different penalty values, this worked the best for the use case\n",
        "            no_repeat_ngram_size=8,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    gen_tokens = out[0][input_len:]\n",
        "    return cleanup_output(tokenizer.decode(gen_tokens, skip_special_tokens=True))\n",
        "\n",
        "TOP_K_FOR_GEN = min(11, TOP_K)\n",
        "\n",
        "for p in policies:\n",
        "    context = build_context_from_idxs(\n",
        "        all_chunks=p[\"chunks\"],\n",
        "        idxs=p[\"retrieved_idxs\"],\n",
        "        max_chunks=TOP_K_FOR_GEN,\n",
        "        per_chunk_chars=1200\n",
        "    )\n",
        "\n",
        "    print(f\"\\n[GEN] url={p['url']} | context_chunks={min(TOP_K_FOR_GEN, len(set(p['retrieved_idxs'])))}\")\n",
        "    simplified = generate_rewrite(context, max_new_tokens=GEN_MAX_NEW_TOKENS).strip()\n",
        "\n",
        "    simplified += (\n",
        "        \"\\n\\n---\\n\"\n",
        "        \"Note (experimental): This summary is for learning/testing purposes only and may contain mistakes. \"\n",
        "        \"The official and legally binding text is the original privacy policy.\\n\"\n",
        "        \"Ahmed Wadee Moustafa\"\n",
        "    )\n",
        "\n",
        "    p[\"simplified\"] = simplified\n",
        "\n",
        "    print(\"\\n\" + \"=\"*90)\n",
        "    print(\"SIMPLIFIED OUTPUT\")\n",
        "    print(\"=\"*90)\n",
        "    print(simplified)\n",
        "    print(\"=\"*90 + \"\\n\")\n"
      ],
      "metadata": {
        "id": "71Ufx894NgLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I ran the **trial summary** and as a normal user Id rate it as ( proof, not perfect)\n",
        "The **good part** is it matched the real chunks: **location (approx + GPS)**, **image/audio analysis**, **sharing with ads/analytics/payment**, and **data transfer** (Singapore/Malaysia/Ireland/US)\n",
        "The **bad part** is small **over-talk / semi-hallucsination**: it used **bullet points** even tho we said “short paragrphs only”, and it wrote “you **consent**” which can be risky if the text didnt say it cleary\n",
        "Also some lines are **too general** like “business partners for social networking”, feels broad and not super pinned to the exact wording\n"
      ],
      "metadata": {
        "id": "0WpeSP1SPj_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluation"
      ],
      "metadata": {
        "id": "u_TuxcK8VKwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_scores(text: str) -> dict:\n",
        "    return {\n",
        "        \"FKGL\": float(textstat.flesch_kincaid_grade(text)),\n",
        "        \"GunningFog\": float(textstat.gunning_fog(text)),\n",
        "        \"SMOG\": float(textstat.smog_index(text)),\n",
        "        \"Words\": int(len(text.split())),\n",
        "        \"Chars\": int(len(text)),\n",
        "    }\n",
        "\n",
        "rows = []\n",
        "for p in policies:\n",
        "    before = p[\"text\"]\n",
        "    after = p[\"simplified\"]\n",
        "\n",
        "    s_before = compute_scores(before)\n",
        "    s_after = compute_scores(after)\n",
        "\n",
        "    rows.append({\"Policy\": p[\"url\"], \"Version\": \"Before\", **s_before})\n",
        "    rows.append({\"Policy\": p[\"url\"], \"Version\": \"After\", **s_after})\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "display(df)\n",
        "\n",
        "metrics = [\"FKGL\", \"GunningFog\", \"SMOG\"]\n",
        "\n",
        "for policy_url in df[\"Policy\"].unique():\n",
        "    sub = df[df[\"Policy\"] == policy_url].set_index(\"Version\")\n",
        "    before_vals = [sub.loc[\"Before\", m] for m in metrics]\n",
        "    after_vals = [sub.loc[\"After\", m] for m in metrics]\n",
        "\n",
        "    x = np.arange(len(metrics))\n",
        "    w = 0.35\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.bar(x - w/2, before_vals, width=w, label=\"Before\")\n",
        "    plt.bar(x + w/2, after_vals, width=w, label=\"After\")\n",
        "    plt.xticks(x, metrics)\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.title(f\"Readability Metrics (Before vs After)\\n{policy_url}\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "fPmeNMxePlYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I checked the **scores table** and its clear the output is waaay shorter than the real policy\n",
        "The original is **8237 words**, but the rewrite is only **374 words**, so it’s more like a tiny summary not a full simplifed policy\n",
        "Readability got a bit better (**FKGL / Fog / SMOG** drop), but it’s still kinda high, so the language is still “legal-ish”\n",
        "So for now it’s ok as a **trial proof**, but later I need either **more tokens** or a **chunk-by-chunk rewrite** or even more maybe even paid models and gpu power if I want more lenght\n"
      ],
      "metadata": {
        "id": "SZLha-v4Q0s9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Method 2 (OpenAI) — Full Policy Simplification (rewrite, NOT a short summary)\n",
        "# Goal: keep ALL meaning + details, but rewrite in very simple English.\n",
        "#\n",
        "# Why chunk-by-chunk?\n",
        "# - One-shot on the full policy can hit context limits or silently truncate.\n",
        "# - Chunking keeps coverage stable.\n",
        "# =========================\n",
        "\n",
        "!pip -q install -U openai\n",
        "from openai import OpenAI\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"OPENAI_API_KEY: \")\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "avail = {m.id for m in client.models.list().data}\n",
        "PREF  = [\"gpt-5.2-pro\",\"gpt-5.2\",\"gpt-5\",\"gpt-4.1\",\"gpt-4o\",\"gpt-4o-mini\"]\n",
        "MODEL = next((m for m in PREF if m in avail), None)\n",
        "if not MODEL:\n",
        "    raise RuntimeError(\"No preferred model available (billing/access).\")\n",
        "\n",
        "MAX_OUT = 1400\n",
        "\n",
        "PROMPT = (\n",
        "    \"You are a detailed and precise legal language simplifier.\\n\"\n",
        "    \"Rewrite the policy text below in VERY simple English (A1 level max).\\n\"\n",
        "    \"Preserve ALL legal meaning and ALL details. Do NOT turn it into a short summary.\\n\"\n",
        "    \"Strict rules:\\n\"\n",
        "    \"- Do not add, guess, or invent anything.\\n\"\n",
        "    \"- Do not omit details.\\n\"\n",
        "    \"- Do not mix different parts in a way that changes meaning.\\n\"\n",
        "    \"- Keep the original order as much as possible.\\n\"\n",
        "    \"- No bullet points. No numbering.\\n\"\n",
        "    \"- Avoid weird symbols or anything that looks like code.\\n\"\n",
        "    \"- Use short sentences and short paragraphs.\\n\"\n",
        "    \"- If a legal term is needed to keep meaning, keep it and explain it simply.\\n\\n\"\n",
        "    \"TEXT:\\n{chunk}\"\n",
        ")\n",
        "\n",
        "def _rw(chunk: str) -> str:\n",
        "    r = client.responses.create(\n",
        "        model=MODEL,\n",
        "        max_output_tokens=MAX_OUT,\n",
        "        input=[{\"role\":\"user\",\"content\":PROMPT.format(chunk=chunk)}],\n",
        "    )\n",
        "    return (r.output_text or \"\").strip()\n",
        "\n",
        "def _print_head_mid_tail(title: str, txt: str, head=2000, mid=2000, tail=2000):\n",
        "    print(\"\\n\" + \"=\"*90)\n",
        "    print(f\"{title} (head)\")\n",
        "    print(\"=\"*90)\n",
        "    print(txt[:head])\n",
        "\n",
        "    m = len(txt)//2\n",
        "    print(\"\\n\" + \"-\"*90)\n",
        "    print(f\"{title} (middle)\")\n",
        "    print(\"-\"*90)\n",
        "    print(txt[max(0, m-mid//2): m+mid//2])\n",
        "\n",
        "    print(\"\\n\" + \"-\"*90)\n",
        "    print(f\"{title} (tail)\")\n",
        "    print(\"-\"*90)\n",
        "    print(txt[-tail:])\n",
        "\n",
        "for p in policies:\n",
        "    chunks = p.get(\"chunks_clean\") or p.get(\"chunks\") or chunk_text(\n",
        "        p[\"text\"], CHUNK_WORDS, CHUNK_OVERLAP, MIN_CHUNK_WORDS\n",
        "    )\n",
        "\n",
        "    p[\"simplified_openai_full\"] = \"\\n\\n\".join(\n",
        "        _rw(c) for c in chunks if c and c.strip()\n",
        "    ).strip()\n",
        "\n",
        "    # BEFORE vs AFTER (head + middle + tail)\n",
        "    _print_head_mid_tail(\"BEFORE\", p[\"text\"])\n",
        "    _print_head_mid_tail(\"AFTER\",  p[\"simplified_openai_full\"])\n",
        "\n",
        "    # metrics table (kept as-is)\n",
        "    df = pd.DataFrame([\n",
        "        {\"Policy\": p[\"url\"], \"Version\": \"Before\", **compute_scores(p[\"text\"])},\n",
        "        {\"Policy\": p[\"url\"], \"Version\": \"After_RAG_Small\", **compute_scores(p.get(\"simplified\",\"\"))},\n",
        "        {\"Policy\": p[\"url\"], \"Version\": \"After_OpenAI_Full\", **compute_scores(p[\"simplified_openai_full\"])}\n",
        "    ])\n",
        "    display(df)\n",
        "\n",
        "    print(\"MODEL:\", MODEL)\n",
        "    print(\"words before:\", len(p[\"text\"].split()))\n",
        "    print(\"words rag_small:\", len(p.get(\"simplified\",\"\").split()))\n",
        "    print(\"words openai_full:\", len(p[\"simplified_openai_full\"].split()))\n"
      ],
      "metadata": {
        "id": "waB3NDbh6B-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    # ONE graph only (same style: 3 metrics, 3 versions)\n",
        "    metrics = [\"FKGL\", \"GunningFog\", \"SMOG\"]\n",
        "    sub = df.set_index(\"Version\")\n",
        "\n",
        "    before_vals = [sub.loc[\"Before\", m] for m in metrics]\n",
        "    rag_vals    = [sub.loc[\"After_RAG_Small\", m] for m in metrics]\n",
        "    full_vals   = [sub.loc[\"After_OpenAI_Full\", m] for m in metrics]\n",
        "\n",
        "    x = np.arange(len(metrics))\n",
        "    w = 0.28\n",
        "\n",
        "    plt.figure(figsize=(9, 4))\n",
        "    plt.bar(x - w, before_vals, width=w, label=\"Before\")\n",
        "    plt.bar(x,     rag_vals,    width=w, label=\"After_RAG_Small\")\n",
        "    plt.bar(x + w, full_vals,   width=w, label=\"After_OpenAI_Full\")\n",
        "    plt.xticks(x, metrics)\n",
        "    plt.ylabel(\"Score (lower is easier)\")\n",
        "    plt.title(f\"Readability Scores — {p['url']}\")\n",
        "    plt.savefig(f\"outputs/readability_scores_{i}.png\", dpi=200, bbox_inches=\"tight\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "rzPPnwZom4ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion (what the results really mean)\n",
        "\n",
        "I can now say this PoC worked in the way i actually needed, not just “some output”. The original TikTok policy is about **8237 words**, and the reading level was crazy high (**FKGL ~14.82**, **Fog ~17.66**). So even if a normal user tries, they will miss stuff or get bored fast, because the language is legal and dense.\n",
        "\n",
        "When i used the Top-K RAG path, the output was only **374 words**. Thats not real “simplification”, thats more like a **small summary** of a few important areas. It can be useful for a fast view (like data sharing, ads, location, transfers), but it does not cover the full policy, so i cant claim it solves the real problem alone.\n",
        "\n",
        "The real shift happened with the OpenAI chunk-by-chunk rewrite. It kept **full coverage**, and still made the policy easier to read. The output became **13045 words**, and thats not a bug. Its logical. Legal sentences got split into smaller sentences, and hard terms got **explained** inside the text, so the meaning stays but the reading becomes easier. The scores prove that too: **FKGL dropped to ~8.07**, **Fog to ~10.31**, and **SMOG to ~10.67**. So i got a big readability win without shrinking the policy into a tiny “marketing” summary.\n",
        "\n",
        "I also did the eye check (head / middle / tail) and the flow looks consistent. The rewrite kept the same idea order and didnt look like random invented paragraphs. So for this PoC, the best logic is: i use chunk-rewrite as the main method for the **full simplified policy**, and i keep Top-K retrieval as a second method only for **quick risk highlights** or “where to look first”, not as a replacment for full simplifcation.\n"
      ],
      "metadata": {
        "id": "LEW5fB66VNMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"outputs\", exist_ok=True)\n",
        "\n",
        "for i,p in enumerate(policies,1):\n",
        "    open(f\"outputs/policy_{i}_before.txt\",\"w\",encoding=\"utf-8\").write(p[\"text\"])\n",
        "    open(f\"outputs/policy_{i}_rag_small.txt\",\"w\",encoding=\"utf-8\").write(p.get(\"simplified\",\"\"))\n",
        "    open(f\"outputs/policy_{i}_openai_full.txt\",\"w\",encoding=\"utf-8\").write(p.get(\"simplified_openai_full\",\"\"))\n",
        "\n",
        "print(\"saved txt files -> outputs/\")\n"
      ],
      "metadata": {
        "id": "_BvkBrXYiEwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wrap-up\n",
        "\n",
        "I finished the main run and the results look stable.\n",
        "\n",
        "- **Model used (OpenAI):** gpt-5.2-pro  \n",
        "- **Policy size (original):** ~8237 words  \n",
        "- **Runtime (this run):** ~24 minutes for 1 policy (end-to-end, chunk rewrite)  \n",
        "- **Cost note:** this method makes many model calls (one per chunk), so scaling to more policies will **cost more** and will likely need a bigger budget (and maybe stronger compute / batching) to stay fast.\n",
        "\n",
        "### What i saved\n",
        "- The full **Before** text\n",
        "- The full **After (OpenAI full rewrite)** text\n",
        "- The **RAG small** output (for quick risk highlights)\n",
        "- A small JSON summary with **word counts + readability scores**\n",
        "- The one readability chart image (Before vs RAG vs OpenAI)\n",
        "\n",
        "### Next step if i scale to more policies\n",
        "If i run this on 5 policies, i should expect:\n",
        "- higher total cost (more chunks = more calls)\n",
        "- longer runtime (unless i parallelize or reduce per-chunk output)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vJoC0fo2gScm"
      }
    }
  ]
}